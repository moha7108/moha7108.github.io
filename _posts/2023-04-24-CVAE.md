---
title: Attribute to Image (CVAE)
author: Mohamed Debbagh
layout: post
icon: fa-solid fa-eye
---
**Topic:** Learning Structured Output Representations from Attributes using Deep Conditional Generative Models

**Abstract** â€” Structured output representation is a generative task explored in 
computer vision that often times requires the mapping of low dimensional 
features to high dimensional structured outputs. Losses in complex spatial 
information in deterministic approaches such as Convolutional Neural Networks 
(CNN) lead to uncertainties and ambiguous structures within a single output 
representation. A probabilistic approach through deep Conditional Generative 
Models (CGM) is presented by Sohn et al. in which a particular model known 
as the Conditional Variational Auto-encoder (CVAE) is introduced and explored. 
While the original paper focuses on the task of image segmentation, 
this paper adopts the CVAE framework for the task of controlled output 
representation through attributes. This approach allows us to learn a 
disentangled multimodal prior distribution, resulting in more controlled 
and robust approach to sample generation. In this work we recreate the 
CVAE architecture and train it on images conditioned on various attributes 
obtained from two image datasets; the Large-scale CelebFaces 
Attributes (CelebA) dataset and the Caltech-UCSD Birds (CUB-200-2011) dataset. We attempt 
to generate new faces with distinct attributes such as hair color and glasses, 
as well as different bird species samples with various attributes. We further
introduce strategies for improving generalized sample generation by applying 
a weighted term to the variational lower bound.

### Lecture slides:

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://docs.google.com/presentation/d/e/2PACX-1vSsbmKOLcm0Y3kSmO2Iyj76pJZNOuj9v3K2x5O0BYFXWcaK-l8ex586Pr44z5yEYEZdSk7RSevOSpdf/embed?start=true&loop=true&delayms=5000' frameborder='0' width='1440' height='839' allowfullscreen='true' mozallowfullscreen='true' webkitallowfullscreen='true'></iframe></div>
<br/>

### Paper:

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><object data='/assets/papers/2023-04-24_attr_to_img.pdf' type='application/pdf'>     <embed src='/assets/papers/2023-04-24_attr_to_img.pdf'>         <p>This browser does not support PDFs. Please download the PDF to view it: <a href='/assets/papers/2023-04-24_attr_to_img.pdf'>Download PDF</a>.</p>     </embed> </object></div>
<br/>

### Citation (Bibtex):

```
@inproceedings{mildenhall2020nerf,
 title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
 author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
 year={2020},
 booktitle={ECCV},
}
```